{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System based on Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ludivine/opt/anaconda3/lib/python3.8/site-packages/gensim/utils.py:1268: UserWarning: detected OSX with python3.8+; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected %s; aliasing chunkize to chunkize_serial\" % entity)\n"
     ]
    }
   ],
   "source": [
    "wiki = WikiCorpus(\"enwiki-20210101-pages-articles-multistream12.xml-p8554860p9172788.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedWikiDocument(object):\n",
    "    def __init__(self, wiki):\n",
    "        self.wiki = wiki\n",
    "        self.wiki.metadata = True\n",
    "    def __iter__(self):\n",
    "        for content, (page_id, title) in self.wiki.get_texts():\n",
    "            yield TaggedDocument(content, [title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = TaggedWikiDocument(wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre = Doc2Vec(min_count=0)\n",
    "#pre.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for num in range(0, 20):\n",
    "    #print('min_count: {}, size of vocab: '.format(num), pre.scale_vocab(min_count=num, dry_run=True)['memory']['vocab']/700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, dbow_words=1, vector_size=200, window=8, min_count=19, epochs=10, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, vector_size=200, window=8, min_count=19, epochs=10, workers=cores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,n5,w8,mc19,s0.001,t4)\n",
      "Doc2Vec(dm/m,d200,n5,w8,mc19,s0.001,t4)\n"
     ]
    }
   ],
   "source": [
    "models[0].build_vocab(documents)\n",
    "print(str(models[0]))\n",
    "models[1].reset_from(models[0])\n",
    "print(str(models[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 8min 50s, sys: 3min 18s, total: 2h 12min 8s\n",
      "Wall time: 1h 19min 49s\n",
      "CPU times: user 33min 7s, sys: 1min 55s, total: 35min 3s\n",
      "Wall time: 42min 45s\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    %time model.train(documents, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model in models:\n",
    "    #print(str(model))\n",
    "    #pprint(model.docvecs.most_similar(positive=[\"Impractical joker\"], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'to', 'love']\n",
      "[('Love to Love', 0.7007774114608765),\n",
      " ('The Romance of Kenny G', 0.6808857917785645),\n",
      " ('Just Like Heaven', 0.6676369905471802),\n",
      " ('Comfort and Joy', 0.6651524901390076),\n",
      " ('Sour (album)', 0.6620450019836426),\n",
      " ('Talking in Your Sleep', 0.6607742309570312),\n",
      " ('The Very Best of Kenny G', 0.6546471118927002),\n",
      " ('The Collection (Kenny G album)', 0.6523045301437378),\n",
      " ('Meet You There (album)', 0.6512295603752136),\n",
      " ('Soldier (Neil Young song)', 0.6511427760124207)]\n",
      "['love', 'to', 'love']\n",
      "[('Bible Christian Mission', 0.8978371024131775),\n",
      " ('Nuchatlaht First Nation', 0.8909875750541687),\n",
      " ('Saskatchewan Highway 953', 0.8902007341384888),\n",
      " ('Ernest Davies (Stretford MP)', 0.8901641368865967),\n",
      " ('Oudkarspel', 0.8898391723632812),\n",
      " ('Salvador Salguero', 0.8893624544143677),\n",
      " ('List of foliage plant diseases (Bromeliaceae)', 0.8885855674743652),\n",
      " (\"Diving at the 1920 Summer Olympics – Men's plain high diving\",\n",
      "  0.8865423202514648),\n",
      " ('Sepia bidhaia', 0.8862035274505615),\n",
      " ('KCMX-FM', 0.8855456113815308)]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    string= \"love to love\".split()\n",
    "    print(string)\n",
    "    doc_vector = model.infer_vector(string)\n",
    "    pprint(model.docvecs.most_similar(positive=[doc_vector], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a wiki link: https://en.wikipedia.org/wiki/Love_to_Love\n",
      "Love_to_Love\n",
      "https://en.wikipedia.org/wiki/Love_to_Love\n",
      "['love', 'to', 'love']\n",
      "======Model======\n",
      "https://en.wikipedia.org/wiki/The_Romance_of_Kenny_G\n",
      "https://en.wikipedia.org/wiki/Heart_and_Crime\n",
      "https://en.wikipedia.org/wiki/Love_to_Love\n",
      "https://en.wikipedia.org/wiki/Stop_the_Machine\n",
      "https://en.wikipedia.org/wiki/Lost_and_Gone\n",
      "https://en.wikipedia.org/wiki/Back_for_My_Life\n",
      "https://en.wikipedia.org/wiki/Marti_Pellow_Sings_the_Hits_of_Wet_Wet_Wet_&_Smile\n",
      "https://en.wikipedia.org/wiki/The_Very_Best_of_Kenny_G\n",
      "https://en.wikipedia.org/wiki/Will_You_Still_Love_Me?_(EP)\n",
      "https://en.wikipedia.org/wiki/Talking_in_Your_Sleep\n",
      "======Model======\n",
      "https://en.wikipedia.org/wiki/Nuchatlaht_First_Nation\n",
      "https://en.wikipedia.org/wiki/Owen_Township,_Winnebago_County,_Illinois\n",
      "https://en.wikipedia.org/wiki/KCMX-FM\n",
      "https://en.wikipedia.org/wiki/List_of_foliage_plant_diseases_(Bromeliaceae)\n",
      "https://en.wikipedia.org/wiki/Banatska_Dubica\n",
      "https://en.wikipedia.org/wiki/Sepia_bidhaia\n",
      "https://en.wikipedia.org/wiki/Diving_at_the_1920_Summer_Olympics_–_Men's_plain_high_diving\n",
      "https://en.wikipedia.org/wiki/Diving_at_the_1972_Summer_Olympics_–_Women's_10_metre_platform\n",
      "https://en.wikipedia.org/wiki/Rockton_Township,_Winnebago_County,_Illinois\n",
      "https://en.wikipedia.org/wiki/Ernest_Davies_(Stretford_MP)\n"
     ]
    }
   ],
   "source": [
    "link = input (\"Enter a wiki link: \") \n",
    "print(link[30:])\n",
    "print(link)\n",
    "string= str(link[30:]).replace(\"_\", \" \")\n",
    "#string = str(link)\n",
    "string= \"\".join([x.lower() for x in string]).split(\" \")\n",
    "print(string)\n",
    "for model in models:\n",
    "    doc_vector = model.infer_vector(string)\n",
    "    print(\"======Model======\")\n",
    "    for i in range(len(model.docvecs.most_similar(positive=[doc_vector], topn=10))):\n",
    "        new_string= model.docvecs.most_similar(positive=[doc_vector], topn=10)[i][0].replace(\" \", \"_\")\n",
    "        print('https://en.wikipedia.org/wiki/'+new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def process_query(query):\n",
    "    #words = []\n",
    "    #words = query.split()\n",
    "    #return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"Impractical_Joker\"\n",
    "#l = process_query(query)\n",
    "#for model in models:\n",
    "    #sim = model.wv.most_similar(positive=l,topn=10)\n",
    "    #print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender system interface : <br>\n",
    "Link example : <br>\n",
    "- Invalide = https://en.wikipedia.org/wiki/Impractical_Jokers <br>\n",
    "- Valide = https://en.wikipedia.org/wiki/Love_to_Love"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('&')\n",
    "import operator\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('enwiki-20210101-pages-articles-multistream12.xml-p8554860p9172788')\n",
    "root = tree.getroot()\n",
    "\n",
    "titles = []\n",
    "texts = []\n",
    "ids = []\n",
    "\n",
    "ns = {'mediawiki': 'http://www.mediawiki.org/xml/export-0.10/'}\n",
    "for child in root.findall('mediawiki:page', ns):\n",
    "    title = child.find('mediawiki:title', ns)\n",
    "    identifier = child.find('mediawiki:id', ns)\n",
    "    titles.append(title.text)\n",
    "    ids.append(identifier.text)\n",
    "    for revision in child.findall('mediawiki:revision', ns):\n",
    "        text_data = revision.find('mediawiki:text', ns)\n",
    "        if text_data != None:\n",
    "            texts.append(text_data.text)\n",
    "        else:\n",
    "            texts.append(None)\n",
    "\n",
    "dataframe = pd.DataFrame(data={'Title': titles, 'ID': ids, 'Text': texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chestnut Ridge Middle School</td>\n",
       "      <td>8554860</td>\n",
       "      <td>#REDIRECT[[Washington Township Public School D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Colegio de Santa Cruz de Tlatelolco</td>\n",
       "      <td>8554864</td>\n",
       "      <td>{{Infobox university\\n|name              = Col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Impractical joker (garfield)</td>\n",
       "      <td>8554867</td>\n",
       "      <td>#REDIRECT [[List of Garfield and Friends episo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>National Council of Teachers</td>\n",
       "      <td>8554873</td>\n",
       "      <td>'''National Council of Teachers''' may refer t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shuo Wang</td>\n",
       "      <td>8554878</td>\n",
       "      <td>#REDIRECT [[Wang Shuo]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The impractical joker garfield and friends</td>\n",
       "      <td>8554883</td>\n",
       "      <td>#REDIRECT [[List of Garfield and Friends episo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Order of battle at Beiping–Tianjin</td>\n",
       "      <td>8554884</td>\n",
       "      <td>'''Peiking Tientsin Operation''' (July–August ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gulshani</td>\n",
       "      <td>8554885</td>\n",
       "      <td>{{about|the Sufi order|the demonym of Gulshan|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The impractical joker garfield &amp; friends</td>\n",
       "      <td>8554892</td>\n",
       "      <td>#REDIRECT [[List of Garfield and Friends episo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The impractical joker garfield</td>\n",
       "      <td>8554898</td>\n",
       "      <td>#REDIRECT [[List of Garfield and Friends episo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Title       ID  \\\n",
       "0                Chestnut Ridge Middle School  8554860   \n",
       "1         Colegio de Santa Cruz de Tlatelolco  8554864   \n",
       "2                Impractical joker (garfield)  8554867   \n",
       "3                National Council of Teachers  8554873   \n",
       "4                                   Shuo Wang  8554878   \n",
       "5  The impractical joker garfield and friends  8554883   \n",
       "6          Order of battle at Beiping–Tianjin  8554884   \n",
       "7                                    Gulshani  8554885   \n",
       "8    The impractical joker garfield & friends  8554892   \n",
       "9              The impractical joker garfield  8554898   \n",
       "\n",
       "                                                Text  \n",
       "0  #REDIRECT[[Washington Township Public School D...  \n",
       "1  {{Infobox university\\n|name              = Col...  \n",
       "2  #REDIRECT [[List of Garfield and Friends episo...  \n",
       "3  '''National Council of Teachers''' may refer t...  \n",
       "4                            #REDIRECT [[Wang Shuo]]  \n",
       "5  #REDIRECT [[List of Garfield and Friends episo...  \n",
       "6  '''Peiking Tientsin Operation''' (July–August ...  \n",
       "7  {{about|the Sufi order|the demonym of Gulshan|...  \n",
       "8  #REDIRECT [[List of Garfield and Friends episo...  \n",
       "9  #REDIRECT [[List of Garfield and Friends episo...  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_lines = 'Portal|File|Category|JPG|PNG|jpg|Wikipedia|Template'\n",
    "dataframe = dataframe[~dataframe.Title.str.contains(drop_lines)]\n",
    "dataframe = dataframe.dropna().reset_index()\n",
    "del dataframe['index']\n",
    "dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_user():\n",
    "    request = str(input(\"Please enter a Wikipedia page name: \"))\n",
    "    return request\n",
    "\n",
    "\n",
    "def propose_pages(request, titles):\n",
    "    # Preprocessing for request\n",
    "    list_results = []\n",
    "    request = nltk.word_tokenize(request)\n",
    "    request = [x.lower() for x in request]\n",
    "    request = [word for word in request if word not in stopwords]\n",
    "    if len(request) > 1:\n",
    "        request = [lemmatizer.lemmatize(w) for w in request]\n",
    "    else:\n",
    "        request = lemmatizer.lemmatize(request[0])\n",
    "    for el in titles:\n",
    "        el_2 = el.lower()\n",
    "        el_2 = nltk.word_tokenize(el_2)\n",
    "        el_2 = ' '.join([lemmatizer.lemmatize(w) for w in el_2])\n",
    "        if type(request) == list:\n",
    "            for i in request:\n",
    "                if i in el_2:\n",
    "                    list_results.append(el)\n",
    "        else:\n",
    "            if request in el_2:\n",
    "                list_results.append(el)\n",
    "    return list_results[:10]\n",
    "    \n",
    "    \n",
    "def check_validity(dataframe):\n",
    "    request = ask_user()\n",
    "    if \"https://en.wikipedia.org/wiki/\" in request:\n",
    "        while request != 'exit':\n",
    "            print(\"Original title : \"+request[30:])\n",
    "            request = str(request[30:])\n",
    "            request = request.replace(\"_\", \" \")\n",
    "            print(\"Title for searching : \"+request)\n",
    "            if type(request) == str: \n",
    "                print(request)\n",
    "                if request in dataframe['Title'].values:\n",
    "                    print(\"Correct Wikipedia page name, we will propose you 10 related pages!\")\n",
    "                    ##Recommendations\n",
    "                    string= \"\".join([x.lower() for x in request]).split(\" \")\n",
    "                    for model in models:\n",
    "                        doc_vector = model.infer_vector(string)\n",
    "                        print(\"======Model======\")\n",
    "                        for i in range(len(model.docvecs.most_similar(positive=[doc_vector], topn=10))):\n",
    "                            new_string= model.docvecs.most_similar(positive=[doc_vector], topn=10)[i][0].replace(\" \", \"_\")\n",
    "                            print('https://en.wikipedia.org/wiki/'+new_string)\n",
    "                    break\n",
    "                else:\n",
    "                    if len(request) != 0:\n",
    "                        results = propose_pages(request, dataframe['Title'])\n",
    "                        print('\\nIncorrect Wikipedia page, please retry!\\n')\n",
    "                        if len(results) > 0:\n",
    "                            print('Some suggestions :) \\n')\n",
    "                            for i,j in enumerate(results):\n",
    "                                print(str(i)+'. '+j)\n",
    "                                print(\"https://en.wikipedia.org/wiki/\"+j.replace(\" \", \"_\"))\n",
    "                        request = ask_user()\n",
    "    else:\n",
    "        print('\\nIncorrect Wikipedia page, please retry!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a Wikipedia page name: https://en.wikipedia.org/wiki/Love_to_Love\n",
      "Original title : Love_to_Love\n",
      "Title for searching : Love to Love\n",
      "Love to Love\n",
      "Correct Wikipedia page name, we will propose you 10 related pages!\n",
      "======Model======\n",
      "https://en.wikipedia.org/wiki/The_Romance_of_Kenny_G\n",
      "https://en.wikipedia.org/wiki/Love_to_Love\n",
      "https://en.wikipedia.org/wiki/The_Collection_(Kenny_G_album)\n",
      "https://en.wikipedia.org/wiki/Will_You_Still_Love_Me?_(EP)\n",
      "https://en.wikipedia.org/wiki/Lost_and_Gone\n",
      "https://en.wikipedia.org/wiki/Soldier_(Neil_Young_song)\n",
      "https://en.wikipedia.org/wiki/The_Very_Best_of_Kenny_G\n",
      "https://en.wikipedia.org/wiki/Sour_(album)\n",
      "https://en.wikipedia.org/wiki/Just_Like_Heaven\n",
      "https://en.wikipedia.org/wiki/Stop_the_Machine\n",
      "======Model======\n",
      "https://en.wikipedia.org/wiki/Speed_skating_at_the_1999_Asian_Winter_Games\n",
      "https://en.wikipedia.org/wiki/Frankl\n",
      "https://en.wikipedia.org/wiki/Bible_Christian_Mission\n",
      "https://en.wikipedia.org/wiki/Hellstrom\n",
      "https://en.wikipedia.org/wiki/List_of_foliage_plant_diseases_(Bromeliaceae)\n",
      "https://en.wikipedia.org/wiki/Mihajlovo\n",
      "https://en.wikipedia.org/wiki/Durand_Township,_Winnebago_County,_Illinois\n",
      "https://en.wikipedia.org/wiki/Diving_at_the_1920_Summer_Olympics_–_Men's_plain_high_diving\n",
      "https://en.wikipedia.org/wiki/Chetin_Kazak\n",
      "https://en.wikipedia.org/wiki/Diving_at_the_1972_Summer_Olympics_–_Men's_3_metre_springboard\n"
     ]
    }
   ],
   "source": [
    "check_validity(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
